{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72fe5845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n",
      "c:\\Users\\user\\miniconda3\\envs\\ml_yp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from pymystem3 import Mystem\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from gensim.models import word2vec\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import optuna\n",
    "\n",
    "\n",
    "# Добавляем в path вышестоящую директорию для импорта  calc_metrics\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "\n",
    "from utils import calc_metrics\n",
    "from word2vec_utils import Word2VecVectorizer, Word2VecTfIdfVectorizer\n",
    "\n",
    "RANDOM_STATE = 41825352\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2dc792",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"../data/x.csv\", index_col=\"date\")\n",
    "y = pd.read_csv(\"../data/y.csv\", index_col=\"date\").iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77c1b2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# инициализируем лемматизатор\n",
    "mystem = Mystem()\n",
    "\n",
    "# загружаем стоп-слова\n",
    "STOP_WORDS = set(stopwords.words(\"russian\"))\n",
    "\n",
    "\n",
    "# функция для препроцессинга текста\n",
    "def preprocessor(text):\n",
    "\n",
    "    # приводим к нижнему регистру\n",
    "    text = text.lower()\n",
    "\n",
    "    # удаляем все символы, кроме пробелов и русских букв.\n",
    "    regex = re.compile(\"[^а-я А-ЯЁё]\")\n",
    "    text = regex.sub(\" \", text)\n",
    "\n",
    "    # лемматизируем тексты\n",
    "    text = \" \".join(mystem.lemmatize(text))\n",
    "\n",
    "    # удаляем стоп-слова\n",
    "    text = \" \".join([word for word in text.split() if word not in STOP_WORDS])\n",
    "    return text\n",
    "\n",
    "\n",
    "Xpreproc = X.release.apply(preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "0eb4ba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "PADDING_TOKEN = '<pad>'\n",
    "EMBEDDING_SIZE = 50\n",
    "\n",
    "data = Xpreproc.str.split().apply(lambda x: x +[PADDING_TOKEN])\n",
    "w2v = word2vec.Word2Vec(data, window=5, workers=1, sg=0, vector_size=EMBEDDING_SIZE, seed=RANDOM_STATE)\n",
    "\n",
    "max_len = data.apply(len).max()\n",
    "padding_idx = w2v.wv.get_index(PADDING_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "21ccddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gensim\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# https://chriskhanhtran.github.io/posts/cnn-sentence-classification/#31-create-cnn-model\n",
    "\n",
    "class CnnTextClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, window_sizes=(2,3,5)):\n",
    "        super(CnnTextClassifier, self).__init__()\n",
    "        w2vmodel = w2v #gensim.models.KeyedVectors.load('w2v.model')\n",
    "        weights = w2vmodel.wv\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(\n",
    "            torch.FloatTensor(weights.vectors),\n",
    "            padding_idx=w2vmodel.wv.get_index(PADDING_TOKEN)\n",
    "        )\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, 64, [window_size, EMBEDDING_SIZE], padding=(window_size - 1, 0))\n",
    "            for window_size in window_sizes\n",
    "        ])\n",
    "\n",
    "        self.fc = nn.Linear(64 * len(window_sizes), num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "\n",
    "        if len(x.size()) < 4:\n",
    "            x = torch.unsqueeze(x, 1)\n",
    "        xs = []\n",
    "        for conv in self.convs:\n",
    "            x2 = torch.relu(conv(x))\n",
    "            x2 = torch.squeeze(x2, -1)\n",
    "            x2 = F.max_pool1d(x2, x2.size(2))\n",
    "            xs.append(x2)\n",
    "        x = torch.cat(xs, 2)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        logits = self.fc(self.dropout(x))\n",
    "\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "35cf12d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CnnTextClassifier(num_classes=3)\n",
    "cnn_model.to(device)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.0001)\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "ec1d5e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model.train()\n",
    "\n",
    "vec = torch.stack(data.apply(make_word2vec_vector_cnn).tolist()).squeeze(1)\n",
    "probs = cnn_model(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "f882d719",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(label):\n",
    "    return torch.tensor([int(label)+1], dtype=torch.long, device=device)\n",
    "\n",
    "def make_word2vec_vector_cnn(sentence):\n",
    "    padded_X = [padding_idx for i in range(max_len)]\n",
    "    i = 0\n",
    "    for word in sentence:\n",
    "        if word not in w2v.wv:\n",
    "            padded_X[i] = 0\n",
    "        else:\n",
    "            padded_X[i] = w2v.wv.get_index(word)\n",
    "        i += 1\n",
    "    return torch.tensor(padded_X, dtype=torch.long, device=device).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "114e215c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = torch.stack(data.apply(make_word2vec_vector_cnn).tolist()).squeeze(1).to(device)\n",
    "y1 = torch.stack(y.apply(make_target).tolist()).squeeze().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "0e0fe4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [07:37<00:00,  6.63s/it, loss = 0.554999589920044, lr = 0.001] \n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1000\n",
    "\n",
    "y_preds = []\n",
    "y_preds_proba = []\n",
    "\n",
    "prog = tqdm(range(30, len(y)))\n",
    "for threshold in prog:\n",
    "    cnn_model = CnnTextClassifier(num_classes=3)\n",
    "    cnn_model.to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(cnn_model.parameters(), lr=0.001, fused=True)\n",
    "    #scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[1500])\n",
    "    \n",
    "    cnn_model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        cnn_model.zero_grad()\n",
    "        bow_vec = data1[:threshold]\n",
    "        probs = cnn_model(bow_vec)\n",
    "        target = y1[:threshold]\n",
    "        loss = loss_function(probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #scheduler.step()\n",
    "        if epoch % 20 == 0:\n",
    "            prog.set_postfix_str(f'loss = {loss.item()}, lr = {optimizer.param_groups[0]['lr']}')\n",
    "\n",
    "    cnn_model.eval()\n",
    "    bow_vec = make_word2vec_vector_cnn(data[threshold])\n",
    "    probs = cnn_model(bow_vec)\n",
    "    pred = torch.argmax(probs) - 1\n",
    "    y_preds_proba.append(probs.detach().cpu().numpy())\n",
    "    y_preds.append(pred.detach().cpu().numpy().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "df9e3b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5507246376811594"
      ]
     },
     "execution_count": 359,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "\n",
    "accuracy_score(y_preds, y[30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "dc1fb1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7067971862022205"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(\n",
    "    y[30:],\n",
    "    np.concatenate(y_preds_proba, axis=0),\n",
    "    average=\"macro\",\n",
    "    multi_class=\"ovo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "ec0d4000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.57      0.59      0.58        22\n",
      "           0       0.59      0.47      0.52        34\n",
      "           1       0.47      0.69      0.56        13\n",
      "\n",
      "    accuracy                           0.55        69\n",
      "   macro avg       0.54      0.58      0.55        69\n",
      "weighted avg       0.56      0.55      0.55        69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_preds, y[30:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "a6f93421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  8,  1],\n",
       "       [ 9, 16,  9],\n",
       "       [ 1,  3,  9]], dtype=int64)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_preds, y[30:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3197b521",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_yp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

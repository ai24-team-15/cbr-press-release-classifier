{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, recall_score, precision_score,\n",
        "    roc_auc_score, classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "try:\n",
        "    from pymystem3 import Mystem\n",
        "    from nltk.corpus import stopwords\n",
        "    PREPROCESSING_AVAILABLE = True\n",
        "    print(\"Библиотеки для препроцессинга\")\n",
        "except ImportError:\n",
        "    PREPROCESSING_AVAILABLE = False\n",
        "    print(\"Pymystem3 или NLTK недоступны, используем простую обработку\")"
      ],
      "metadata": {
        "id": "9fNxwxN6UPjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ecbbedb-6adc-4beb-9ce8-fe04a5f0c202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Библиотеки для препроцессинга\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "RWx8lxwiUbAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a89d3286-c2cc-496b-fe5a-d31925b51f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Оптимизированная ensemble модель для превышения всех существующих результатов\n",
        "Архитектура: TF-IDF + Feature Engineering + Multiple Algorithms + Stacking\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, recall_score, precision_score,\n",
        "    roc_auc_score, classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "try:\n",
        "    from pymystem3 import Mystem\n",
        "    from nltk.corpus import stopwords\n",
        "    PREPROCESSING_AVAILABLE = True\n",
        "    print(\"Библиотеки для препроцессинга загружены\")\n",
        "except ImportError:\n",
        "    PREPROCESSING_AVAILABLE = False\n",
        "    print(\"Pymystem3 или NLTK недоступны, используем простую обработку\")\n",
        "\n",
        "def load_data():\n",
        "    paths_to_try = [\n",
        "        (\"../data/x.csv\", \"../data/y.csv\"),\n",
        "        (\"./data/x.csv\", \"./data/y.csv\"),\n",
        "        (\"x.csv\", \"y.csv\"),\n",
        "        (\"./x.csv\", \"./y.csv\")\n",
        "    ]\n",
        "\n",
        "    X, y = None, None\n",
        "    for x_path, y_path in paths_to_try:\n",
        "        try:\n",
        "            X=pd.read_csv(x_path, index_col=\"date\")\n",
        "            y=pd.read_csv(y_path, index_col=\"date\").iloc[:, 0]\n",
        "            print(f\"Данные загружены из: {x_path}, {y_path}\")\n",
        "            break\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка загрузки {x_path}: {e}\")\n",
        "            continue\n",
        "    if X is None or y is None:\n",
        "        print(\"ОШИБКА: Файлы данных не найдены!\")\n",
        "        return None, None\n",
        "    print(f\"Загружено {len(X)} образцов\")\n",
        "    print(f\"Распределение классов: {dict(y.value_counts().sort_index())}\")\n",
        "    return X, y\n",
        "\n",
        "def preprocess_texts(texts):\n",
        "    if PREPROCESSING_AVAILABLE:\n",
        "        try:\n",
        "            mystem=Mystem()\n",
        "            stop_words=set(stopwords.words(\"russian\"))\n",
        "            def advanced_preprocessor(text):\n",
        "                if pd.isna(text):\n",
        "                    return \"\"\n",
        "                text = str(text).lower()\n",
        "                text = re.sub(r'[^а-яё\\s]', ' ', text)\n",
        "                lemmatized=mystem.lemmatize(text)\n",
        "                text= ' '.join(lemmatized)\n",
        "                words=[word for word in text.split()\n",
        "                        if word not in stop_words and len(word) >= 2]\n",
        "                return ' '.join(words)\n",
        "            print(\"Продвинутый препроцессинг (Mystem + стоп-слова)\")\n",
        "            return texts.apply(advanced_preprocessor)\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка продвинутого препроцессинга: {e}\")\n",
        "\n",
        "    def simple_preprocessor(text):\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "        text=str(text).lower()\n",
        "        text=re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        text=re.sub(r'\\d+', ' ', text)\n",
        "        text=re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "    print(\"Простой препроцессинг\")\n",
        "    return texts.apply(simple_preprocessor)\n",
        "\n",
        "class AdvancedFeatureExtractor:\n",
        "    def __init__(self):\n",
        "        self.feature_extractors = {}\n",
        "        self.scalers = {}\n",
        "    def extract_linguistic_features(self, texts):\n",
        "        features = []\n",
        "        positive_words = [\n",
        "            'рост', 'увеличение', 'повышение', 'прибыль', 'доход', 'положительный',\n",
        "            'улучшение', 'восстановление', 'стабилизация', 'укрепление', 'развитие'\n",
        "        ]\n",
        "        negative_words = [\n",
        "            'снижение', 'падение', 'уменьшение', 'убыток', 'кризис', 'негативный',\n",
        "            'ухудшение', 'нестабильность', 'спад', 'сокращение', 'риск'\n",
        "        ]\n",
        "        neutral_words = [\n",
        "            'стабильный', 'неизменный', 'нейтральный', 'текущий', 'обычный'\n",
        "            'сохранение', 'поддержание', 'ожидание', 'прогноз'\n",
        "        ]\n",
        "        financial_terms = [\n",
        "            'рубль', 'доллар', 'процент', 'ставка', 'инфляция', 'валюта', 'банк',\n",
        "            'кредит', 'депозит', 'инвестиции', 'капитал', 'ликвидность', 'волатильность'\n",
        "        ]\n",
        "        time_indicators = [\n",
        "            'квартал', 'месяц', 'год', 'период', 'срок', 'временно', 'краткосрочный',\n",
        "            'долгосрочный', 'будущий', 'прошлый', 'текущий'\n",
        "        ]\n",
        "        for text in texts:\n",
        "            if pd.isna(text):\n",
        "                text = \"\"\n",
        "            text_lower = text.lower()\n",
        "            words = text_lower.split()\n",
        "            text_features = {}\n",
        "            text_features['text_length'] = len(text)\n",
        "            text_features['word_count'] = len(words)\n",
        "            text_features['sentence_count'] = len([s for s in text.split('.') if s.strip()])\n",
        "            text_features['avg_word_length'] = np.mean([len(w) for w in words]) if words else 0\n",
        "            text_features['unique_word_ratio'] = len(set(words)) / max(len(words), 1)\n",
        "            text_features['punct_ratio'] = len(re.findall(r'[^\\w\\s]', text)) / max(len(text), 1)\n",
        "            text_features['upper_ratio'] = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n",
        "            text_features['digit_ratio'] = sum(1 for c in text if c.isdigit()) / max(len(text), 1)\n",
        "            text_features['comma_count'] = text.count(',')\n",
        "            text_features['period_count'] = text.count('.')\n",
        "            text_features['exclamation_count'] = text.count('!')\n",
        "            text_features['question_count'] = text.count('?')\n",
        "            text_features['positive_words_count'] = sum(1 for word in positive_words if word in text_lower)\n",
        "            text_features['negative_words_count'] = sum(1 for word in negative_words if word in text_lower)\n",
        "            text_features['neutral_words_count'] = sum(1 for word in neutral_words if word in text_lower)\n",
        "            total_sentiment_words = (text_features['positive_words_count'] +\n",
        "                                   text_features['negative_words_count'] +\n",
        "                                   text_features['neutral_words_count'])\n",
        "            if total_sentiment_words > 0:\n",
        "                text_features['sentiment_ratio'] = (\n",
        "                    text_features['positive_words_count'] - text_features['negative_words_count']\n",
        "                ) / total_sentiment_words\n",
        "                text_features['sentiment_intensity'] = total_sentiment_words / max(len(words), 1)\n",
        "            else:\n",
        "                text_features['sentiment_ratio'] = 0\n",
        "                text_features['sentiment_intensity'] = 0\n",
        "            text_features['financial_terms_count'] = sum(1 for term in financial_terms if term in text_lower)\n",
        "            text_features['financial_density'] = text_features['financial_terms_count'] / max(len(words), 1)\n",
        "            text_features['time_indicators_count'] = sum(1 for term in time_indicators if term in text_lower)\n",
        "            text_features['complex_words_ratio'] = sum(1 for word in words if len(word) > 6) / max(len(words), 1)\n",
        "            text_features['short_words_ratio'] = sum(1 for word in words if len(word) <= 3) / max(len(words), 1)\n",
        "            bigrams=[' '.join(words[i:i+2]) for i in range(len(words)-1)]\n",
        "            trigrams=[' '.join(words[i:i+3]) for i in range(len(words)-2)]\n",
        "            key_bigrams=['процентная ставка', 'денежная политика', 'инфляционные ожидания', 'экономический рост']\n",
        "            key_trigrams= ['ключевая процентная ставка', 'совет директоров банка', 'денежно кредитная политика']\n",
        "            text_features['key_bigrams_count'] = sum(1 for bigram in key_bigrams if bigram in text_lower)\n",
        "            text_features['key_trigrams_count'] = sum(1 for trigram in key_trigrams if trigram in text_lower)\n",
        "            features.append(list(text_features.values()))\n",
        "        return np.array(features)\n",
        "    def fit_transform(self, texts, verbose=False):\n",
        "        if verbose:\n",
        "            print(\"Извлечение признаков...\")\n",
        "        # 1. TF-IDF с разными параметрами\n",
        "        self.feature_extractors['tfidf_1_3'] = TfidfVectorizer(\n",
        "            ngram_range=(1, 3),\n",
        "            max_features=8000,\n",
        "            min_df=2,\n",
        "            max_df=0.8,\n",
        "            sublinear_tf=True\n",
        "        )\n",
        "        self.feature_extractors['tfidf_1_2'] = TfidfVectorizer(\n",
        "            ngram_range=(1, 2),\n",
        "            max_features=5000,\n",
        "            min_df=3,\n",
        "            max_df=0.7,\n",
        "            sublinear_tf=True\n",
        "        )\n",
        "        self.feature_extractors['tfidf_char'] = TfidfVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(2, 5),\n",
        "            max_features=3000,\n",
        "            min_df=2\n",
        "        )\n",
        "        self.feature_extractors['count_vec'] = CountVectorizer(\n",
        "            ngram_range=(1, 2),\n",
        "            max_features=3000,\n",
        "            min_df=3,\n",
        "            max_df=0.8\n",
        "        )\n",
        "        feature_sets={}\n",
        "        for name, extractor in self.feature_extractors.items():\n",
        "            feature_sets[name] = extractor.fit_transform(texts).toarray()\n",
        "            if verbose:\n",
        "                print(f\"     {name}: {feature_sets[name].shape[1]} признаков\")\n",
        "        # Лингвистические признаки\n",
        "        linguistic_features=self.extract_linguistic_features(texts)\n",
        "        feature_sets['linguistic']=linguistic_features\n",
        "        if verbose:\n",
        "            print(f\"linguistic: {linguistic_features.shape[1]} признаков\")\n",
        "        # Нормализация лингвистических признаков\n",
        "        self.scalers['linguistic']=StandardScaler()\n",
        "        feature_sets['linguistic']=self.scalers['linguistic'].fit_transform(feature_sets['linguistic'])\n",
        "        return feature_sets\n",
        "\n",
        "    def transform(self, texts):\n",
        "        feature_sets ={}\n",
        "        for name, extractor in self.feature_extractors.items():\n",
        "            feature_sets[name]=extractor.transform(texts).toarray()\n",
        "        linguistic_features = self.extract_linguistic_features(texts)\n",
        "        feature_sets['linguistic'] = self.scalers['linguistic'].transform(linguistic_features)\n",
        "\n",
        "        return feature_sets\n",
        "\n",
        "class StackingEnsembleClassifier:\n",
        "#Стекинг ансамбль с несколькими уровнями\n",
        "\n",
        "    def __init__(self):\n",
        "        self.base_models={}\n",
        "        self.meta_models={}\n",
        "        self.feature_extractor=AdvancedFeatureExtractor()\n",
        "        self.label_mapping={-1: 0, 0: 1, 1: 2}\n",
        "        self.reverse_mapping ={0: -1, 1: 0, 2: 1}\n",
        "\n",
        "    def _create_base_models(self):\n",
        " #Создание базовых моделей\n",
        "        models = {\n",
        "            # Линейные модели\n",
        "            'lr_balanced': LogisticRegression(\n",
        "                max_iter=1000, C=1.0, class_weight='balanced', random_state=42\n",
        "            ),\n",
        "            'lr_l1': LogisticRegression(\n",
        "                max_iter=1000, C=0.5, penalty='l1', solver='liblinear',\n",
        "                class_weight='balanced', random_state=42\n",
        "            ),\n",
        "            'ridge': RidgeClassifier(\n",
        "                alpha=1.0, class_weight='balanced', random_state=42\n",
        "            ),\n",
        "            # Ensemble модели\n",
        "            'rf_balanced': RandomForestClassifier(\n",
        "                n_estimators=200, max_depth=15, class_weight='balanced',\n",
        "                random_state=42, n_jobs=-1\n",
        "            ),\n",
        "            'gb_classifier': GradientBoostingClassifier(\n",
        "                n_estimators=150, learning_rate=0.1, max_depth=6, random_state=42\n",
        "            ),\n",
        "            # SVM\n",
        "            'svm_rbf': SVC(\n",
        "                kernel='rbf', C=1.0, probability=True, class_weight='balanced', random_state=42\n",
        "            ),\n",
        "            'svm_linear': SVC(\n",
        "                kernel='linear', C=0.5, probability=True, class_weight='balanced', random_state=42\n",
        "            ),\n",
        "            # Naive Bayes\n",
        "            'nb_multinomial': MultinomialNB(alpha=0.1),\n",
        "            # KNN\n",
        "            'knn': KNeighborsClassifier(n_neighbors=7, weights='distance'),\n",
        "            # Neural Network\n",
        "            'mlp': MLPClassifier(\n",
        "                hidden_layer_sizes=(256, 128), activation='relu',\n",
        "                alpha=0.001, max_iter=500, random_state=42\n",
        "            )\n",
        "        }\n",
        "        return models\n",
        "\n",
        "    def fit(self, texts, labels, verbose=False):\n",
        "        \"\"\"Обучение стекинг ансамбля\"\"\"\n",
        "        # Извлечение признаков\n",
        "        feature_sets = self.feature_extractor.fit_transform(texts, verbose=verbose)\n",
        "\n",
        "        # Преобразование меток\n",
        "        y_mapped = [self.label_mapping[label] for label in labels]\n",
        "\n",
        "        # Создание базовых моделей\n",
        "        base_models = self._create_base_models()\n",
        "\n",
        "        model_configs = [\n",
        "            # TF-IDF модели\n",
        "            ('lr_balanced_tfidf_1_3', 'tfidf_1_3', base_models['lr_balanced']),\n",
        "            ('lr_l1_tfidf_1_2', 'tfidf_1_2', base_models['lr_l1']),\n",
        "            ('svm_rbf_tfidf', 'tfidf_1_3', base_models['svm_rbf']),\n",
        "            ('nb_tfidf', 'tfidf_1_2', base_models['nb_multinomial']),\n",
        "            # Char-level модели\n",
        "            ('lr_char', 'tfidf_char', LogisticRegression(max_iter=1000, C=0.1, class_weight='balanced')),\n",
        "            ('svm_char', 'tfidf_char', SVC(kernel='linear', C=0.1, probability=True, class_weight='balanced')),\n",
        "            # Count vectorizer модели\n",
        "            ('rf_count', 'count_vec', base_models['rf_balanced']),\n",
        "            ('gb_count', 'count_vec', base_models['gb_classifier']),\n",
        "            # Модели на лингвистических признаках\n",
        "            ('lr_linguistic', 'linguistic', LogisticRegression(max_iter=1000, C=0.1, class_weight='balanced')),\n",
        "            ('rf_linguistic', 'linguistic', RandomForestClassifier(n_estimators=100, class_weight='balanced')),\n",
        "            ('knn_linguistic', 'linguistic', base_models['knn']),\n",
        "            # Комбинированные признаки\n",
        "            ('mlp_combined', 'combined', base_models['mlp']),\n",
        "            ('rf_combined', 'combined', RandomForestClassifier(n_estimators=150, max_depth=12, class_weight='balanced')),\n",
        "        ]\n",
        "        # Создание комбинированных признаков\n",
        "        main_tfidf = feature_sets['tfidf_1_3']\n",
        "        linguistic = feature_sets['linguistic']\n",
        "        combined_features = np.hstack([main_tfidf, linguistic])\n",
        "        feature_sets['combined'] = combined_features\n",
        "\n",
        "        # Обучение базовых моделей\n",
        "        self.base_models = {}\n",
        "        for model_name, feature_name, model in model_configs:\n",
        "            try:\n",
        "                X_features = feature_sets[feature_name]\n",
        "                model.fit(X_features, y_mapped)\n",
        "                self.base_models[model_name] = (model, feature_name)\n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(f\"Ошибка обучения {model_name}: {e}\")\n",
        "        meta_features=[]\n",
        "        for model_name, (model, feature_name) in self.base_models.items():\n",
        "            X_features=feature_sets[feature_name]\n",
        "            try:\n",
        "                proba=model.predict_proba(X_features)\n",
        "                meta_features.append(proba)\n",
        "            except:\n",
        "\n",
        "                pred= model.predict(X_features)\n",
        "                proba =np.eye(3)[pred]\n",
        "                meta_features.append(proba)\n",
        "        if meta_features:\n",
        "            meta_X = np.hstack(meta_features)\n",
        "            self.meta_models = {\n",
        "                'meta_lr': LogisticRegression(max_iter=1000, C=0.1, random_state=42),\n",
        "                'meta_rf': RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42),\n",
        "                'meta_gb': GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, random_state=42)\n",
        "            }\n",
        "            for name, model in self.meta_models.items():\n",
        "                model.fit(meta_X, y_mapped)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, texts):\n",
        "#Предсказание вероятностей\n",
        "        feature_sets=self.feature_extractor.transform(texts)\n",
        "        main_tfidf=feature_sets['tfidf_1_3']\n",
        "        linguistic=feature_sets['linguistic']\n",
        "        combined_features=np.hstack([main_tfidf, linguistic])\n",
        "        feature_sets['combined'] = combined_features\n",
        "        # Получение предсказаний от базовых моделей\n",
        "        meta_features = []\n",
        "        for model_name, (model, feature_name) in self.base_models.items():\n",
        "            X_features = feature_sets[feature_name]\n",
        "            try:\n",
        "                proba = model.predict_proba(X_features)\n",
        "                meta_features.append(proba)\n",
        "            except:\n",
        "                pred=model.predict(X_features)\n",
        "                proba=np.eye(3)[pred]\n",
        "                meta_features.append(proba)\n",
        "        if meta_features:\n",
        "            meta_X=np.hstack(meta_features)\n",
        "\n",
        "            meta_predictions=[]\n",
        "            for name, model in self.meta_models.items():\n",
        "                meta_pred=model.predict_proba(meta_X)\n",
        "                meta_predictions.append(meta_pred)\n",
        "\n",
        "            final_proba=np.mean(meta_predictions, axis=0)\n",
        "            return final_proba\n",
        "        else:\n",
        "            return np.full((len(texts), 3), 1/3)\n",
        "\n",
        "    def predict(self, texts):\n",
        "        #Предсказание классов\n",
        "        probas=self.predict_proba(texts)\n",
        "        predictions=np.argmax(probas, axis=1)\n",
        "        return [self.reverse_mapping[pred] for pred in predictions]\n",
        "\n",
        "def run_advanced_ensemble_experiment(X, y):\n",
        "    X_preprocessed = preprocess_texts(X.release)\n",
        "    print(f\"Препроцессинг завершен для {len(X_preprocessed)} текстов\")\n",
        "    predictions=[]\n",
        "    verbose_step=max(1, (len(X_preprocessed) - 30) // 5)\n",
        "    print(f\"\\n Обучение на {len(X_preprocessed) - 30} итерациях...\")\n",
        "    for i in tqdm(range(30, len(X_preprocessed)), desc=\"Обучение модели\"):\n",
        "        try:\n",
        "            # Обучающие данные\n",
        "            train_texts=X_preprocessed.iloc[:i].tolist()\n",
        "            train_labels y.iloc[:i].tolist()\n",
        "            # Создание и обучение ансамбля\n",
        "            ensemble = StackingEnsembleClassifier()\n",
        "            show_verbose = (i - 30) % verbose_step == 0 and (i - 30) < verbose_step * 2\n",
        "            ensemble.fit(train_texts, train_labels, verbose=show_verbose)\n",
        "\n",
        "            # Предсказание\n",
        "            test_text=[X_preprocessed.iloc[i]]\n",
        "            y_pred_proba=ensemble.predict_proba(test_text)[0]\n",
        "            y_pred=ensemble.predict(test_text)[0]\n",
        "            predictions.append((y_pred, y_pred_proba))\n",
        "        except Exception as e:\n",
        "            print(f\"\\nОшибка на итерации {i}: {e}\")\n",
        "            predictions.append((0, np.array([0.33, 0.34, 0.33])))\n",
        "    return predictions\n",
        "\n",
        "def calculate_and_display_metrics(predictions, y_true):\n",
        "    \"\"\"Расчет и вывод метрик\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"РЕЗУЛЬТАТЫ ADVANCED STACKING ENSEMBLE МОДЕЛИ\")\n",
        "    print(\"=\"*80)\n",
        "    # Извлечение предсказаний и вероятностей\n",
        "    y_preds=[pred[0] for pred in predictions]\n",
        "    y_preds_proba=np.array([pred[1] for pred in predictions])\n",
        "    # Обрезка до одинаковой длины\n",
        "    min_len=min(len(y_true), len(y_preds))\n",
        "    y_true=y_true[:min_len]\n",
        "    y_preds=y_preds[:min_len]\n",
        "    y_preds_proba=y_preds_proba[:min_len]\n",
        "\n",
        "    # Расчет всех метрик\n",
        "    accuracy=accuracy_score(y_true, y_preds)\n",
        "    f1=f1_score(y_true, y_preds, average='macro')\n",
        "    recall=recall_score(y_true, y_preds, average='macro')\n",
        "    precision=precision_score(y_true, y_preds, average='macro')\n",
        "\n",
        "    try:\n",
        "        roc_auc_ovr=roc_auc_score(y_true, y_preds_proba, average='macro', multi_class='ovr')\n",
        "        roc_auc_ovo=roc_auc_score(y_true, y_preds_proba, average='macro', multi_class='ovo')\n",
        "    except:\n",
        "        roc_auc_ovr=0.5\n",
        "        roc_auc_ovo=0.5\n",
        "\n",
        "    # Вывод основных метрик\n",
        "    print(f\"\\nОСНОВНЫЕ МЕТРИКИ ADVANCED ENSEMBLE:\")\n",
        "    print(f\"Accuracy:     {accuracy:.6f}\")\n",
        "    print(f\"F1-score:     {f1:.6f}\")\n",
        "    print(f\"Recall:       {recall:.6f}\")\n",
        "    print(f\"Precision:    {precision:.6f}\")\n",
        "    print(f\"ROC-AUC OvR:  {roc_auc_ovr:.6f}\")\n",
        "    print(f\"ROC-AUC OvO:  {roc_auc_ovo:.6f}\")\n",
        "\n",
        "    # Детальный отчет по классам\n",
        "    print(f\"\\nCLASSIFICATION REPORT:\")\n",
        "    print(classification_report(y_true, y_preds,\n",
        "                              target_names=['Negative (-1)', 'Neutral (0)', 'Positive (1)']))\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'recall': recall,\n",
        "        'precision': precision,\n",
        "        'roc_auc_ovr': roc_auc_ovr,\n",
        "        'roc_auc_ovo': roc_auc_ovo,\n",
        "        'predictions': predictions\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    print(\"ЗАПУСК ADVANCED STACKING ENSEMBLE ЭКСПЕРИМЕНТА\")\n",
        "    print(\"=\"*80)\n",
        "    # 1. Загрузка данных\n",
        "    X, y = load_data()\n",
        "    if X is None or y is None:\n",
        "        print(\"Не удалось загрузить данные. Завершение работы.\")\n",
        "        return None\n",
        "\n",
        "    # 2. Запуск эксперимента\n",
        "    try:\n",
        "        predictions = run_advanced_ensemble_experiment(X, y)\n",
        "        print(f\"Получено {len(predictions)} предсказаний\")\n",
        "\n",
        "        # 3. Расчет и вывод метрик\n",
        "        y_true = y.iloc[30:30+len(predictions)].tolist()\n",
        "        results = calculate_and_display_metrics(predictions, y_true)\n",
        "\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка во время эксперимента: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "# Запуск эксперимента\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()\n",
        "else:\n",
        "    print(\"Advanced Stacking Ensemble модель готова к запуску!\")\n",
        "    print(\"Для запуска выполните: results = main()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IpN7XQ3ENJg",
        "outputId": "5499a3a6-d153-4d39-88e3-8cdd1706162d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Библиотеки для препроцессинга загружены\n",
            "ЗАПУСК ADVANCED STACKING ENSEMBLE ЭКСПЕРИМЕНТА\n",
            "================================================================================\n",
            "Данные загружены из: x.csv, y.csv\n",
            "📊 Загружено 100 образцов\n",
            "📈 Распределение классов: {-1.0: np.int64(30), 0.0: np.int64(45), 1.0: np.int64(25)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Продвинутый препроцессинг (Mystem + стоп-слова)\n",
            "Препроцессинг завершен для 100 текстов\n",
            "\n",
            "🔄 Обучение на 70 итерациях...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rОбучение модели:   0%|          | 0/70 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Извлечение признаков...\n",
            "     tfidf_1_3: 4512 признаков\n",
            "     tfidf_1_2: 1541 признаков\n",
            "     tfidf_char: 3000 признаков\n",
            "     count_vec: 1601 признаков\n",
            "linguistic: 24 признаков\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Обучение модели:  20%|██        | 14/70 [02:32<12:17, 13.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Извлечение признаков...\n",
            "     tfidf_1_3: 6740 признаков\n",
            "     tfidf_1_2: 2237 признаков\n",
            "     tfidf_char: 3000 признаков\n",
            "     count_vec: 2266 признаков\n",
            "linguistic: 24 признаков\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Обучение модели: 100%|██████████| 70/70 [19:02<00:00, 16.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Получено 70 предсказаний\n",
            "\n",
            "================================================================================\n",
            "  РЕЗУЛЬТАТЫ ADVANCED STACKING ENSEMBLE МОДЕЛИ\n",
            "================================================================================\n",
            "\n",
            "🚀 ОСНОВНЫЕ МЕТРИКИ ADVANCED ENSEMBLE:\n",
            "   🎯 Accuracy:     0.614286\n",
            "   📈 F1-score:     0.621076\n",
            "   🔄 Recall:       0.609704\n",
            "   🎪 Precision:    0.650350\n",
            "   📊 ROC-AUC OvR:  0.729279\n",
            "   📊 ROC-AUC OvO:  0.735330\n",
            "\n",
            "📋 CLASSIFICATION REPORT:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Negative (-1)       0.67      0.70      0.68        23\n",
            "  Neutral (0)       0.52      0.61      0.56        28\n",
            " Positive (1)       0.77      0.53      0.62        19\n",
            "\n",
            "     accuracy                           0.61        70\n",
            "    macro avg       0.65      0.61      0.62        70\n",
            " weighted avg       0.63      0.61      0.62        70\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ансамбль моделей не дал существенно лучшего результата, но при этом находится ближе к середине (по сравнению с остальными моделями) со значением Accuracy: 0.614286\n",
        "МОдель хорошо отрабатывает длинные и короткие тексты и сложную лексику( благодоря левел и ленгвистическим признакам). Возможно требуется поработать с лингивстическими признаками и n-граммами."
      ],
      "metadata": {
        "id": "XfQr9vIKDUQv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5hHFTPvzEc9G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
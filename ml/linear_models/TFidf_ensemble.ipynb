{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, recall_score, precision_score,\n",
        "    roc_auc_score, classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "try:\n",
        "    from pymystem3 import Mystem\n",
        "    from nltk.corpus import stopwords\n",
        "    PREPROCESSING_AVAILABLE = True\n",
        "    print(\"Ð‘Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³Ð°\")\n",
        "except ImportError:\n",
        "    PREPROCESSING_AVAILABLE = False\n",
        "    print(\"Pymystem3 Ð¸Ð»Ð¸ NLTK Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿Ñ€Ð¾ÑÑ‚ÑƒÑŽ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ\")"
      ],
      "metadata": {
        "id": "9fNxwxN6UPjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ecbbedb-6adc-4beb-9ce8-fe04a5f0c202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ð‘Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³Ð°\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "RWx8lxwiUbAN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a89d3286-c2cc-496b-fe5a-d31925b51f51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "ÐžÐ¿Ñ‚Ð¸Ð¼Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ ensemble Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ²Ñ‹ÑˆÐµÐ½Ð¸Ñ Ð²ÑÐµÑ… ÑÑƒÑ‰ÐµÑÑ‚Ð²ÑƒÑŽÑ‰Ð¸Ñ… Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð²\n",
        "ÐÑ€Ñ…Ð¸Ñ‚ÐµÐºÑ‚ÑƒÑ€Ð°: TF-IDF + Feature Engineering + Multiple Algorithms + Stacking\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, recall_score, precision_score,\n",
        "    roc_auc_score, classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "try:\n",
        "    from pymystem3 import Mystem\n",
        "    from nltk.corpus import stopwords\n",
        "    PREPROCESSING_AVAILABLE = True\n",
        "    print(\"Ð‘Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³Ð° Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹\")\n",
        "except ImportError:\n",
        "    PREPROCESSING_AVAILABLE = False\n",
        "    print(\"Pymystem3 Ð¸Ð»Ð¸ NLTK Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿Ð½Ñ‹, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ Ð¿Ñ€Ð¾ÑÑ‚ÑƒÑŽ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÑƒ\")\n",
        "\n",
        "def load_data():\n",
        "    paths_to_try = [\n",
        "        (\"../data/x.csv\", \"../data/y.csv\"),\n",
        "        (\"./data/x.csv\", \"./data/y.csv\"),\n",
        "        (\"x.csv\", \"y.csv\"),\n",
        "        (\"./x.csv\", \"./y.csv\")\n",
        "    ]\n",
        "\n",
        "    X, y = None, None\n",
        "    for x_path, y_path in paths_to_try:\n",
        "        try:\n",
        "            X=pd.read_csv(x_path, index_col=\"date\")\n",
        "            y=pd.read_csv(y_path, index_col=\"date\").iloc[:, 0]\n",
        "            print(f\"Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ð¸Ð·: {x_path}, {y_path}\")\n",
        "            break\n",
        "        except FileNotFoundError:\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"ÐžÑˆÐ¸Ð±ÐºÐ° Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ {x_path}: {e}\")\n",
        "            continue\n",
        "    if X is None or y is None:\n",
        "        print(\"ÐžÐ¨Ð˜Ð‘ÐšÐ: Ð¤Ð°Ð¹Ð»Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹!\")\n",
        "        return None, None\n",
        "    print(f\"Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ {len(X)} Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð²\")\n",
        "    print(f\"Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÐºÐ»Ð°ÑÑÐ¾Ð²: {dict(y.value_counts().sort_index())}\")\n",
        "    return X, y\n",
        "\n",
        "def preprocess_texts(texts):\n",
        "    if PREPROCESSING_AVAILABLE:\n",
        "        try:\n",
        "            mystem=Mystem()\n",
        "            stop_words=set(stopwords.words(\"russian\"))\n",
        "            def advanced_preprocessor(text):\n",
        "                if pd.isna(text):\n",
        "                    return \"\"\n",
        "                text = str(text).lower()\n",
        "                text = re.sub(r'[^Ð°-ÑÑ‘\\s]', ' ', text)\n",
        "                lemmatized=mystem.lemmatize(text)\n",
        "                text= ' '.join(lemmatized)\n",
        "                words=[word for word in text.split()\n",
        "                        if word not in stop_words and len(word) >= 2]\n",
        "                return ' '.join(words)\n",
        "            print(\"ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ð¹ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³ (Mystem + ÑÑ‚Ð¾Ð¿-ÑÐ»Ð¾Ð²Ð°)\")\n",
        "            return texts.apply(advanced_preprocessor)\n",
        "        except Exception as e:\n",
        "            print(f\"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ñ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ð¾Ð³Ð¾ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³Ð°: {e}\")\n",
        "\n",
        "    def simple_preprocessor(text):\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "        text=str(text).lower()\n",
        "        text=re.sub(r'[^\\w\\s]', ' ', text)\n",
        "        text=re.sub(r'\\d+', ' ', text)\n",
        "        text=re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "    print(\"ÐŸÑ€Ð¾ÑÑ‚Ð¾Ð¹ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³\")\n",
        "    return texts.apply(simple_preprocessor)\n",
        "\n",
        "class AdvancedFeatureExtractor:\n",
        "    def __init__(self):\n",
        "        self.feature_extractors = {}\n",
        "        self.scalers = {}\n",
        "    def extract_linguistic_features(self, texts):\n",
        "        features = []\n",
        "        positive_words = [\n",
        "            'Ñ€Ð¾ÑÑ‚', 'ÑƒÐ²ÐµÐ»Ð¸Ñ‡ÐµÐ½Ð¸Ðµ', 'Ð¿Ð¾Ð²Ñ‹ÑˆÐµÐ½Ð¸Ðµ', 'Ð¿Ñ€Ð¸Ð±Ñ‹Ð»ÑŒ', 'Ð´Ð¾Ñ…Ð¾Ð´', 'Ð¿Ð¾Ð»Ð¾Ð¶Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ð¹',\n",
        "            'ÑƒÐ»ÑƒÑ‡ÑˆÐµÐ½Ð¸Ðµ', 'Ð²Ð¾ÑÑÑ‚Ð°Ð½Ð¾Ð²Ð»ÐµÐ½Ð¸Ðµ', 'ÑÑ‚Ð°Ð±Ð¸Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ', 'ÑƒÐºÑ€ÐµÐ¿Ð»ÐµÐ½Ð¸Ðµ', 'Ñ€Ð°Ð·Ð²Ð¸Ñ‚Ð¸Ðµ'\n",
        "        ]\n",
        "        negative_words = [\n",
        "            'ÑÐ½Ð¸Ð¶ÐµÐ½Ð¸Ðµ', 'Ð¿Ð°Ð´ÐµÐ½Ð¸Ðµ', 'ÑƒÐ¼ÐµÐ½ÑŒÑˆÐµÐ½Ð¸Ðµ', 'ÑƒÐ±Ñ‹Ñ‚Ð¾Ðº', 'ÐºÑ€Ð¸Ð·Ð¸Ñ', 'Ð½ÐµÐ³Ð°Ñ‚Ð¸Ð²Ð½Ñ‹Ð¹',\n",
        "            'ÑƒÑ…ÑƒÐ´ÑˆÐµÐ½Ð¸Ðµ', 'Ð½ÐµÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ', 'ÑÐ¿Ð°Ð´', 'ÑÐ¾ÐºÑ€Ð°Ñ‰ÐµÐ½Ð¸Ðµ', 'Ñ€Ð¸ÑÐº'\n",
        "        ]\n",
        "        neutral_words = [\n",
        "            'ÑÑ‚Ð°Ð±Ð¸Ð»ÑŒÐ½Ñ‹Ð¹', 'Ð½ÐµÐ¸Ð·Ð¼ÐµÐ½Ð½Ñ‹Ð¹', 'Ð½ÐµÐ¹Ñ‚Ñ€Ð°Ð»ÑŒÐ½Ñ‹Ð¹', 'Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹', 'Ð¾Ð±Ñ‹Ñ‡Ð½Ñ‹Ð¹'\n",
        "            'ÑÐ¾Ñ…Ñ€Ð°Ð½ÐµÐ½Ð¸Ðµ', 'Ð¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð°Ð½Ð¸Ðµ', 'Ð¾Ð¶Ð¸Ð´Ð°Ð½Ð¸Ðµ', 'Ð¿Ñ€Ð¾Ð³Ð½Ð¾Ð·'\n",
        "        ]\n",
        "        financial_terms = [\n",
        "            'Ñ€ÑƒÐ±Ð»ÑŒ', 'Ð´Ð¾Ð»Ð»Ð°Ñ€', 'Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚', 'ÑÑ‚Ð°Ð²ÐºÐ°', 'Ð¸Ð½Ñ„Ð»ÑÑ†Ð¸Ñ', 'Ð²Ð°Ð»ÑŽÑ‚Ð°', 'Ð±Ð°Ð½Ðº',\n",
        "            'ÐºÑ€ÐµÐ´Ð¸Ñ‚', 'Ð´ÐµÐ¿Ð¾Ð·Ð¸Ñ‚', 'Ð¸Ð½Ð²ÐµÑÑ‚Ð¸Ñ†Ð¸Ð¸', 'ÐºÐ°Ð¿Ð¸Ñ‚Ð°Ð»', 'Ð»Ð¸ÐºÐ²Ð¸Ð´Ð½Ð¾ÑÑ‚ÑŒ', 'Ð²Ð¾Ð»Ð°Ñ‚Ð¸Ð»ÑŒÐ½Ð¾ÑÑ‚ÑŒ'\n",
        "        ]\n",
        "        time_indicators = [\n",
        "            'ÐºÐ²Ð°Ñ€Ñ‚Ð°Ð»', 'Ð¼ÐµÑÑÑ†', 'Ð³Ð¾Ð´', 'Ð¿ÐµÑ€Ð¸Ð¾Ð´', 'ÑÑ€Ð¾Ðº', 'Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ð¾', 'ÐºÑ€Ð°Ñ‚ÐºÐ¾ÑÑ€Ð¾Ñ‡Ð½Ñ‹Ð¹',\n",
        "            'Ð´Ð¾Ð»Ð³Ð¾ÑÑ€Ð¾Ñ‡Ð½Ñ‹Ð¹', 'Ð±ÑƒÐ´ÑƒÑ‰Ð¸Ð¹', 'Ð¿Ñ€Ð¾ÑˆÐ»Ñ‹Ð¹', 'Ñ‚ÐµÐºÑƒÑ‰Ð¸Ð¹'\n",
        "        ]\n",
        "        for text in texts:\n",
        "            if pd.isna(text):\n",
        "                text = \"\"\n",
        "            text_lower = text.lower()\n",
        "            words = text_lower.split()\n",
        "            text_features = {}\n",
        "            text_features['text_length'] = len(text)\n",
        "            text_features['word_count'] = len(words)\n",
        "            text_features['sentence_count'] = len([s for s in text.split('.') if s.strip()])\n",
        "            text_features['avg_word_length'] = np.mean([len(w) for w in words]) if words else 0\n",
        "            text_features['unique_word_ratio'] = len(set(words)) / max(len(words), 1)\n",
        "            text_features['punct_ratio'] = len(re.findall(r'[^\\w\\s]', text)) / max(len(text), 1)\n",
        "            text_features['upper_ratio'] = sum(1 for c in text if c.isupper()) / max(len(text), 1)\n",
        "            text_features['digit_ratio'] = sum(1 for c in text if c.isdigit()) / max(len(text), 1)\n",
        "            text_features['comma_count'] = text.count(',')\n",
        "            text_features['period_count'] = text.count('.')\n",
        "            text_features['exclamation_count'] = text.count('!')\n",
        "            text_features['question_count'] = text.count('?')\n",
        "            text_features['positive_words_count'] = sum(1 for word in positive_words if word in text_lower)\n",
        "            text_features['negative_words_count'] = sum(1 for word in negative_words if word in text_lower)\n",
        "            text_features['neutral_words_count'] = sum(1 for word in neutral_words if word in text_lower)\n",
        "            total_sentiment_words = (text_features['positive_words_count'] +\n",
        "                                   text_features['negative_words_count'] +\n",
        "                                   text_features['neutral_words_count'])\n",
        "            if total_sentiment_words > 0:\n",
        "                text_features['sentiment_ratio'] = (\n",
        "                    text_features['positive_words_count'] - text_features['negative_words_count']\n",
        "                ) / total_sentiment_words\n",
        "                text_features['sentiment_intensity'] = total_sentiment_words / max(len(words), 1)\n",
        "            else:\n",
        "                text_features['sentiment_ratio'] = 0\n",
        "                text_features['sentiment_intensity'] = 0\n",
        "            text_features['financial_terms_count'] = sum(1 for term in financial_terms if term in text_lower)\n",
        "            text_features['financial_density'] = text_features['financial_terms_count'] / max(len(words), 1)\n",
        "            text_features['time_indicators_count'] = sum(1 for term in time_indicators if term in text_lower)\n",
        "            text_features['complex_words_ratio'] = sum(1 for word in words if len(word) > 6) / max(len(words), 1)\n",
        "            text_features['short_words_ratio'] = sum(1 for word in words if len(word) <= 3) / max(len(words), 1)\n",
        "            bigrams=[' '.join(words[i:i+2]) for i in range(len(words)-1)]\n",
        "            trigrams=[' '.join(words[i:i+3]) for i in range(len(words)-2)]\n",
        "            key_bigrams=['Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ð½Ð°Ñ ÑÑ‚Ð°Ð²ÐºÐ°', 'Ð´ÐµÐ½ÐµÐ¶Ð½Ð°Ñ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ°', 'Ð¸Ð½Ñ„Ð»ÑÑ†Ð¸Ð¾Ð½Ð½Ñ‹Ðµ Ð¾Ð¶Ð¸Ð´Ð°Ð½Ð¸Ñ', 'ÑÐºÐ¾Ð½Ð¾Ð¼Ð¸Ñ‡ÐµÑÐºÐ¸Ð¹ Ñ€Ð¾ÑÑ‚']\n",
        "            key_trigrams= ['ÐºÐ»ÑŽÑ‡ÐµÐ²Ð°Ñ Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ð½Ð°Ñ ÑÑ‚Ð°Ð²ÐºÐ°', 'ÑÐ¾Ð²ÐµÑ‚ Ð´Ð¸Ñ€ÐµÐºÑ‚Ð¾Ñ€Ð¾Ð² Ð±Ð°Ð½ÐºÐ°', 'Ð´ÐµÐ½ÐµÐ¶Ð½Ð¾ ÐºÑ€ÐµÐ´Ð¸Ñ‚Ð½Ð°Ñ Ð¿Ð¾Ð»Ð¸Ñ‚Ð¸ÐºÐ°']\n",
        "            text_features['key_bigrams_count'] = sum(1 for bigram in key_bigrams if bigram in text_lower)\n",
        "            text_features['key_trigrams_count'] = sum(1 for trigram in key_trigrams if trigram in text_lower)\n",
        "            features.append(list(text_features.values()))\n",
        "        return np.array(features)\n",
        "    def fit_transform(self, texts, verbose=False):\n",
        "        if verbose:\n",
        "            print(\"Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²...\")\n",
        "        # 1. TF-IDF Ñ Ñ€Ð°Ð·Ð½Ñ‹Ð¼Ð¸ Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð°Ð¼Ð¸\n",
        "        self.feature_extractors['tfidf_1_3'] = TfidfVectorizer(\n",
        "            ngram_range=(1, 3),\n",
        "            max_features=8000,\n",
        "            min_df=2,\n",
        "            max_df=0.8,\n",
        "            sublinear_tf=True\n",
        "        )\n",
        "        self.feature_extractors['tfidf_1_2'] = TfidfVectorizer(\n",
        "            ngram_range=(1, 2),\n",
        "            max_features=5000,\n",
        "            min_df=3,\n",
        "            max_df=0.7,\n",
        "            sublinear_tf=True\n",
        "        )\n",
        "        self.feature_extractors['tfidf_char'] = TfidfVectorizer(\n",
        "            analyzer='char',\n",
        "            ngram_range=(2, 5),\n",
        "            max_features=3000,\n",
        "            min_df=2\n",
        "        )\n",
        "        self.feature_extractors['count_vec'] = CountVectorizer(\n",
        "            ngram_range=(1, 2),\n",
        "            max_features=3000,\n",
        "            min_df=3,\n",
        "            max_df=0.8\n",
        "        )\n",
        "        feature_sets={}\n",
        "        for name, extractor in self.feature_extractors.items():\n",
        "            feature_sets[name] = extractor.fit_transform(texts).toarray()\n",
        "            if verbose:\n",
        "                print(f\"     {name}: {feature_sets[name].shape[1]} Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\")\n",
        "        # Ð›Ð¸Ð½Ð³Ð²Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸\n",
        "        linguistic_features=self.extract_linguistic_features(texts)\n",
        "        feature_sets['linguistic']=linguistic_features\n",
        "        if verbose:\n",
        "            print(f\"linguistic: {linguistic_features.shape[1]} Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\")\n",
        "        # ÐÐ¾Ñ€Ð¼Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð»Ð¸Ð½Ð³Ð²Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
        "        self.scalers['linguistic']=StandardScaler()\n",
        "        feature_sets['linguistic']=self.scalers['linguistic'].fit_transform(feature_sets['linguistic'])\n",
        "        return feature_sets\n",
        "\n",
        "    def transform(self, texts):\n",
        "        feature_sets ={}\n",
        "        for name, extractor in self.feature_extractors.items():\n",
        "            feature_sets[name]=extractor.transform(texts).toarray()\n",
        "        linguistic_features = self.extract_linguistic_features(texts)\n",
        "        feature_sets['linguistic'] = self.scalers['linguistic'].transform(linguistic_features)\n",
        "\n",
        "        return feature_sets\n",
        "\n",
        "class StackingEnsembleClassifier:\n",
        "#Ð¡Ñ‚ÐµÐºÐ¸Ð½Ð³ Ð°Ð½ÑÐ°Ð¼Ð±Ð»ÑŒ Ñ Ð½ÐµÑÐºÐ¾Ð»ÑŒÐºÐ¸Ð¼Ð¸ ÑƒÑ€Ð¾Ð²Ð½ÑÐ¼Ð¸\n",
        "\n",
        "    def __init__(self):\n",
        "        self.base_models={}\n",
        "        self.meta_models={}\n",
        "        self.feature_extractor=AdvancedFeatureExtractor()\n",
        "        self.label_mapping={-1: 0, 0: 1, 1: 2}\n",
        "        self.reverse_mapping ={0: -1, 1: 0, 2: 1}\n",
        "\n",
        "    def _create_base_models(self):\n",
        " #Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹\n",
        "        models = {\n",
        "            # Ð›Ð¸Ð½ÐµÐ¹Ð½Ñ‹Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
        "            'lr_balanced': LogisticRegression(\n",
        "                max_iter=1000, C=1.0, class_weight='balanced', random_state=42\n",
        "            ),\n",
        "            'lr_l1': LogisticRegression(\n",
        "                max_iter=1000, C=0.5, penalty='l1', solver='liblinear',\n",
        "                class_weight='balanced', random_state=42\n",
        "            ),\n",
        "            'ridge': RidgeClassifier(\n",
        "                alpha=1.0, class_weight='balanced', random_state=42\n",
        "            ),\n",
        "            # Ensemble Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
        "            'rf_balanced': RandomForestClassifier(\n",
        "                n_estimators=200, max_depth=15, class_weight='balanced',\n",
        "                random_state=42, n_jobs=-1\n",
        "            ),\n",
        "            'gb_classifier': GradientBoostingClassifier(\n",
        "                n_estimators=150, learning_rate=0.1, max_depth=6, random_state=42\n",
        "            ),\n",
        "            # SVM\n",
        "            'svm_rbf': SVC(\n",
        "                kernel='rbf', C=1.0, probability=True, class_weight='balanced', random_state=42\n",
        "            ),\n",
        "            'svm_linear': SVC(\n",
        "                kernel='linear', C=0.5, probability=True, class_weight='balanced', random_state=42\n",
        "            ),\n",
        "            # Naive Bayes\n",
        "            'nb_multinomial': MultinomialNB(alpha=0.1),\n",
        "            # KNN\n",
        "            'knn': KNeighborsClassifier(n_neighbors=7, weights='distance'),\n",
        "            # Neural Network\n",
        "            'mlp': MLPClassifier(\n",
        "                hidden_layer_sizes=(256, 128), activation='relu',\n",
        "                alpha=0.001, max_iter=500, random_state=42\n",
        "            )\n",
        "        }\n",
        "        return models\n",
        "\n",
        "    def fit(self, texts, labels, verbose=False):\n",
        "        \"\"\"ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ ÑÑ‚ÐµÐºÐ¸Ð½Ð³ Ð°Ð½ÑÐ°Ð¼Ð±Ð»Ñ\"\"\"\n",
        "        # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
        "        feature_sets = self.feature_extractor.fit_transform(texts, verbose=verbose)\n",
        "\n",
        "        # ÐŸÑ€ÐµÐ¾Ð±Ñ€Ð°Ð·Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¼ÐµÑ‚Ð¾Ðº\n",
        "        y_mapped = [self.label_mapping[label] for label in labels]\n",
        "\n",
        "        # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹\n",
        "        base_models = self._create_base_models()\n",
        "\n",
        "        model_configs = [\n",
        "            # TF-IDF Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
        "            ('lr_balanced_tfidf_1_3', 'tfidf_1_3', base_models['lr_balanced']),\n",
        "            ('lr_l1_tfidf_1_2', 'tfidf_1_2', base_models['lr_l1']),\n",
        "            ('svm_rbf_tfidf', 'tfidf_1_3', base_models['svm_rbf']),\n",
        "            ('nb_tfidf', 'tfidf_1_2', base_models['nb_multinomial']),\n",
        "            # Char-level Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
        "            ('lr_char', 'tfidf_char', LogisticRegression(max_iter=1000, C=0.1, class_weight='balanced')),\n",
        "            ('svm_char', 'tfidf_char', SVC(kernel='linear', C=0.1, probability=True, class_weight='balanced')),\n",
        "            # Count vectorizer Ð¼Ð¾Ð´ÐµÐ»Ð¸\n",
        "            ('rf_count', 'count_vec', base_models['rf_balanced']),\n",
        "            ('gb_count', 'count_vec', base_models['gb_classifier']),\n",
        "            # ÐœÐ¾Ð´ÐµÐ»Ð¸ Ð½Ð° Ð»Ð¸Ð½Ð³Ð²Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ°Ñ…\n",
        "            ('lr_linguistic', 'linguistic', LogisticRegression(max_iter=1000, C=0.1, class_weight='balanced')),\n",
        "            ('rf_linguistic', 'linguistic', RandomForestClassifier(n_estimators=100, class_weight='balanced')),\n",
        "            ('knn_linguistic', 'linguistic', base_models['knn']),\n",
        "            # ÐšÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¸\n",
        "            ('mlp_combined', 'combined', base_models['mlp']),\n",
        "            ('rf_combined', 'combined', RandomForestClassifier(n_estimators=150, max_depth=12, class_weight='balanced')),\n",
        "        ]\n",
        "        # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ ÐºÐ¾Ð¼Ð±Ð¸Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ñ… Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
        "        main_tfidf = feature_sets['tfidf_1_3']\n",
        "        linguistic = feature_sets['linguistic']\n",
        "        combined_features = np.hstack([main_tfidf, linguistic])\n",
        "        feature_sets['combined'] = combined_features\n",
        "\n",
        "        # ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹\n",
        "        self.base_models = {}\n",
        "        for model_name, feature_name, model in model_configs:\n",
        "            try:\n",
        "                X_features = feature_sets[feature_name]\n",
        "                model.fit(X_features, y_mapped)\n",
        "                self.base_models[model_name] = (model, feature_name)\n",
        "            except Exception as e:\n",
        "                if verbose:\n",
        "                    print(f\"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ {model_name}: {e}\")\n",
        "        meta_features=[]\n",
        "        for model_name, (model, feature_name) in self.base_models.items():\n",
        "            X_features=feature_sets[feature_name]\n",
        "            try:\n",
        "                proba=model.predict_proba(X_features)\n",
        "                meta_features.append(proba)\n",
        "            except:\n",
        "\n",
        "                pred= model.predict(X_features)\n",
        "                proba =np.eye(3)[pred]\n",
        "                meta_features.append(proba)\n",
        "        if meta_features:\n",
        "            meta_X = np.hstack(meta_features)\n",
        "            self.meta_models = {\n",
        "                'meta_lr': LogisticRegression(max_iter=1000, C=0.1, random_state=42),\n",
        "                'meta_rf': RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42),\n",
        "                'meta_gb': GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, random_state=42)\n",
        "            }\n",
        "            for name, model in self.meta_models.items():\n",
        "                model.fit(meta_X, y_mapped)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, texts):\n",
        "#ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÐµÐ¹\n",
        "        feature_sets=self.feature_extractor.transform(texts)\n",
        "        main_tfidf=feature_sets['tfidf_1_3']\n",
        "        linguistic=feature_sets['linguistic']\n",
        "        combined_features=np.hstack([main_tfidf, linguistic])\n",
        "        feature_sets['combined'] = combined_features\n",
        "        # ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹ Ð¾Ñ‚ Ð±Ð°Ð·Ð¾Ð²Ñ‹Ñ… Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹\n",
        "        meta_features = []\n",
        "        for model_name, (model, feature_name) in self.base_models.items():\n",
        "            X_features = feature_sets[feature_name]\n",
        "            try:\n",
        "                proba = model.predict_proba(X_features)\n",
        "                meta_features.append(proba)\n",
        "            except:\n",
        "                pred=model.predict(X_features)\n",
        "                proba=np.eye(3)[pred]\n",
        "                meta_features.append(proba)\n",
        "        if meta_features:\n",
        "            meta_X=np.hstack(meta_features)\n",
        "\n",
        "            meta_predictions=[]\n",
        "            for name, model in self.meta_models.items():\n",
        "                meta_pred=model.predict_proba(meta_X)\n",
        "                meta_predictions.append(meta_pred)\n",
        "\n",
        "            final_proba=np.mean(meta_predictions, axis=0)\n",
        "            return final_proba\n",
        "        else:\n",
        "            return np.full((len(texts), 3), 1/3)\n",
        "\n",
        "    def predict(self, texts):\n",
        "        #ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ ÐºÐ»Ð°ÑÑÐ¾Ð²\n",
        "        probas=self.predict_proba(texts)\n",
        "        predictions=np.argmax(probas, axis=1)\n",
        "        return [self.reverse_mapping[pred] for pred in predictions]\n",
        "\n",
        "def run_advanced_ensemble_experiment(X, y):\n",
        "    X_preprocessed = preprocess_texts(X.release)\n",
        "    print(f\"ÐŸÑ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ Ð´Ð»Ñ {len(X_preprocessed)} Ñ‚ÐµÐºÑÑ‚Ð¾Ð²\")\n",
        "    predictions=[]\n",
        "    verbose_step=max(1, (len(X_preprocessed) - 30) // 5)\n",
        "    print(f\"\\n ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð½Ð° {len(X_preprocessed) - 30} Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸ÑÑ…...\")\n",
        "    for i in tqdm(range(30, len(X_preprocessed)), desc=\"ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸\"):\n",
        "        try:\n",
        "            # ÐžÐ±ÑƒÑ‡Ð°ÑŽÑ‰Ð¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ\n",
        "            train_texts=X_preprocessed.iloc[:i].tolist()\n",
        "            train_labels y.iloc[:i].tolist()\n",
        "            # Ð¡Ð¾Ð·Ð´Ð°Ð½Ð¸Ðµ Ð¸ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð°Ð½ÑÐ°Ð¼Ð±Ð»Ñ\n",
        "            ensemble = StackingEnsembleClassifier()\n",
        "            show_verbose = (i - 30) % verbose_step == 0 and (i - 30) < verbose_step * 2\n",
        "            ensemble.fit(train_texts, train_labels, verbose=show_verbose)\n",
        "\n",
        "            # ÐŸÑ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ðµ\n",
        "            test_text=[X_preprocessed.iloc[i]]\n",
        "            y_pred_proba=ensemble.predict_proba(test_text)[0]\n",
        "            y_pred=ensemble.predict(test_text)[0]\n",
        "            predictions.append((y_pred, y_pred_proba))\n",
        "        except Exception as e:\n",
        "            print(f\"\\nÐžÑˆÐ¸Ð±ÐºÐ° Ð½Ð° Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸Ð¸ {i}: {e}\")\n",
        "            predictions.append((0, np.array([0.33, 0.34, 0.33])))\n",
        "    return predictions\n",
        "\n",
        "def calculate_and_display_metrics(predictions, y_true):\n",
        "    \"\"\"Ð Ð°ÑÑ‡ÐµÑ‚ Ð¸ Ð²Ñ‹Ð²Ð¾Ð´ Ð¼ÐµÑ‚Ñ€Ð¸Ðº\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢Ð« ADVANCED STACKING ENSEMBLE ÐœÐžÐ”Ð•Ð›Ð˜\")\n",
        "    print(\"=\"*80)\n",
        "    # Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹ Ð¸ Ð²ÐµÑ€Ð¾ÑÑ‚Ð½Ð¾ÑÑ‚ÐµÐ¹\n",
        "    y_preds=[pred[0] for pred in predictions]\n",
        "    y_preds_proba=np.array([pred[1] for pred in predictions])\n",
        "    # ÐžÐ±Ñ€ÐµÐ·ÐºÐ° Ð´Ð¾ Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²Ð¾Ð¹ Ð´Ð»Ð¸Ð½Ñ‹\n",
        "    min_len=min(len(y_true), len(y_preds))\n",
        "    y_true=y_true[:min_len]\n",
        "    y_preds=y_preds[:min_len]\n",
        "    y_preds_proba=y_preds_proba[:min_len]\n",
        "\n",
        "    # Ð Ð°ÑÑ‡ÐµÑ‚ Ð²ÑÐµÑ… Ð¼ÐµÑ‚Ñ€Ð¸Ðº\n",
        "    accuracy=accuracy_score(y_true, y_preds)\n",
        "    f1=f1_score(y_true, y_preds, average='macro')\n",
        "    recall=recall_score(y_true, y_preds, average='macro')\n",
        "    precision=precision_score(y_true, y_preds, average='macro')\n",
        "\n",
        "    try:\n",
        "        roc_auc_ovr=roc_auc_score(y_true, y_preds_proba, average='macro', multi_class='ovr')\n",
        "        roc_auc_ovo=roc_auc_score(y_true, y_preds_proba, average='macro', multi_class='ovo')\n",
        "    except:\n",
        "        roc_auc_ovr=0.5\n",
        "        roc_auc_ovo=0.5\n",
        "\n",
        "    # Ð’Ñ‹Ð²Ð¾Ð´ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ñ… Ð¼ÐµÑ‚Ñ€Ð¸Ðº\n",
        "    print(f\"\\nÐžÐ¡ÐÐžÐ’ÐÐ«Ð• ÐœÐ•Ð¢Ð Ð˜ÐšÐ˜ ADVANCED ENSEMBLE:\")\n",
        "    print(f\"Accuracy:     {accuracy:.6f}\")\n",
        "    print(f\"F1-score:     {f1:.6f}\")\n",
        "    print(f\"Recall:       {recall:.6f}\")\n",
        "    print(f\"Precision:    {precision:.6f}\")\n",
        "    print(f\"ROC-AUC OvR:  {roc_auc_ovr:.6f}\")\n",
        "    print(f\"ROC-AUC OvO:  {roc_auc_ovo:.6f}\")\n",
        "\n",
        "    # Ð”ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð¿Ð¾ ÐºÐ»Ð°ÑÑÐ°Ð¼\n",
        "    print(f\"\\nCLASSIFICATION REPORT:\")\n",
        "    print(classification_report(y_true, y_preds,\n",
        "                              target_names=['Negative (-1)', 'Neutral (0)', 'Positive (1)']))\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'f1': f1,\n",
        "        'recall': recall,\n",
        "        'precision': precision,\n",
        "        'roc_auc_ovr': roc_auc_ovr,\n",
        "        'roc_auc_ovo': roc_auc_ovo,\n",
        "        'predictions': predictions\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    print(\"Ð—ÐÐŸÐ£Ð¡Ðš ADVANCED STACKING ENSEMBLE Ð­ÐšÐ¡ÐŸÐ•Ð Ð˜ÐœÐ•ÐÐ¢Ð\")\n",
        "    print(\"=\"*80)\n",
        "    # 1. Ð—Ð°Ð³Ñ€ÑƒÐ·ÐºÐ° Ð´Ð°Ð½Ð½Ñ‹Ñ…\n",
        "    X, y = load_data()\n",
        "    if X is None or y is None:\n",
        "        print(\"ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ð·Ð°Ð³Ñ€ÑƒÐ·Ð¸Ñ‚ÑŒ Ð´Ð°Ð½Ð½Ñ‹Ðµ. Ð—Ð°Ð²ÐµÑ€ÑˆÐµÐ½Ð¸Ðµ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹.\")\n",
        "        return None\n",
        "\n",
        "    # 2. Ð—Ð°Ð¿ÑƒÑÐº ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°\n",
        "    try:\n",
        "        predictions = run_advanced_ensemble_experiment(X, y)\n",
        "        print(f\"ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ {len(predictions)} Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹\")\n",
        "\n",
        "        # 3. Ð Ð°ÑÑ‡ÐµÑ‚ Ð¸ Ð²Ñ‹Ð²Ð¾Ð´ Ð¼ÐµÑ‚Ñ€Ð¸Ðº\n",
        "        y_true = y.iloc[30:30+len(predictions)].tolist()\n",
        "        results = calculate_and_display_metrics(predictions, y_true)\n",
        "\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"ÐžÑˆÐ¸Ð±ÐºÐ° Ð²Ð¾ Ð²Ñ€ÐµÐ¼Ñ ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "# Ð—Ð°Ð¿ÑƒÑÐº ÑÐºÑÐ¿ÐµÑ€Ð¸Ð¼ÐµÐ½Ñ‚Ð°\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()\n",
        "else:\n",
        "    print(\"Advanced Stacking Ensemble Ð¼Ð¾Ð´ÐµÐ»ÑŒ Ð³Ð¾Ñ‚Ð¾Ð²Ð° Ðº Ð·Ð°Ð¿ÑƒÑÐºÑƒ!\")\n",
        "    print(\"Ð”Ð»Ñ Ð·Ð°Ð¿ÑƒÑÐºÐ° Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ñ‚Ðµ: results = main()\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2IpN7XQ3ENJg",
        "outputId": "5499a3a6-d153-4d39-88e3-8cdd1706162d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ð‘Ð¸Ð±Ð»Ð¸Ð¾Ñ‚ÐµÐºÐ¸ Ð´Ð»Ñ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³Ð° Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹\n",
            "Ð—ÐÐŸÐ£Ð¡Ðš ADVANCED STACKING ENSEMBLE Ð­ÐšÐ¡ÐŸÐ•Ð Ð˜ÐœÐ•ÐÐ¢Ð\n",
            "================================================================================\n",
            "Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ð¸Ð·: x.csv, y.csv\n",
            "ðŸ“Š Ð—Ð°Ð³Ñ€ÑƒÐ¶ÐµÐ½Ð¾ 100 Ð¾Ð±Ñ€Ð°Ð·Ñ†Ð¾Ð²\n",
            "ðŸ“ˆ Ð Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÐµÐ½Ð¸Ðµ ÐºÐ»Ð°ÑÑÐ¾Ð²: {-1.0: np.int64(30), 0.0: np.int64(45), 1.0: np.int64(25)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing mystem to /root/.local/bin/mystem from http://download.cdn.yandex.net/mystem/mystem-3.1-linux-64bit.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÐŸÑ€Ð¾Ð´Ð²Ð¸Ð½ÑƒÑ‚Ñ‹Ð¹ Ð¿Ñ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³ (Mystem + ÑÑ‚Ð¾Ð¿-ÑÐ»Ð¾Ð²Ð°)\n",
            "ÐŸÑ€ÐµÐ¿Ñ€Ð¾Ñ†ÐµÑÑÐ¸Ð½Ð³ Ð·Ð°Ð²ÐµÑ€ÑˆÐµÐ½ Ð´Ð»Ñ 100 Ñ‚ÐµÐºÑÑ‚Ð¾Ð²\n",
            "\n",
            "ðŸ”„ ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð½Ð° 70 Ð¸Ñ‚ÐµÑ€Ð°Ñ†Ð¸ÑÑ…...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸:   0%|          | 0/70 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²...\n",
            "     tfidf_1_3: 4512 Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
            "     tfidf_1_2: 1541 Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
            "     tfidf_char: 3000 Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
            "     count_vec: 1601 Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
            "linguistic: 24 Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸:  20%|â–ˆâ–ˆ        | 14/70 [02:32<12:17, 13.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Ð˜Ð·Ð²Ð»ÐµÑ‡ÐµÐ½Ð¸Ðµ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²...\n",
            "     tfidf_1_3: 6740 Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
            "     tfidf_1_2: 2237 Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
            "     tfidf_char: 3000 Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
            "     count_vec: 2266 Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n",
            "linguistic: 24 Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ¾Ð²\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ÐžÐ±ÑƒÑ‡ÐµÐ½Ð¸Ðµ Ð¼Ð¾Ð´ÐµÐ»Ð¸: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 70/70 [19:02<00:00, 16.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ÐŸÐ¾Ð»ÑƒÑ‡ÐµÐ½Ð¾ 70 Ð¿Ñ€ÐµÐ´ÑÐºÐ°Ð·Ð°Ð½Ð¸Ð¹\n",
            "\n",
            "================================================================================\n",
            "  Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢Ð« ADVANCED STACKING ENSEMBLE ÐœÐžÐ”Ð•Ð›Ð˜\n",
            "================================================================================\n",
            "\n",
            "ðŸš€ ÐžÐ¡ÐÐžÐ’ÐÐ«Ð• ÐœÐ•Ð¢Ð Ð˜ÐšÐ˜ ADVANCED ENSEMBLE:\n",
            "   ðŸŽ¯ Accuracy:     0.614286\n",
            "   ðŸ“ˆ F1-score:     0.621076\n",
            "   ðŸ”„ Recall:       0.609704\n",
            "   ðŸŽª Precision:    0.650350\n",
            "   ðŸ“Š ROC-AUC OvR:  0.729279\n",
            "   ðŸ“Š ROC-AUC OvO:  0.735330\n",
            "\n",
            "ðŸ“‹ CLASSIFICATION REPORT:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "Negative (-1)       0.67      0.70      0.68        23\n",
            "  Neutral (0)       0.52      0.61      0.56        28\n",
            " Positive (1)       0.77      0.53      0.62        19\n",
            "\n",
            "     accuracy                           0.61        70\n",
            "    macro avg       0.65      0.61      0.62        70\n",
            " weighted avg       0.63      0.61      0.62        70\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ÐÐ½ÑÐ°Ð¼Ð±Ð»ÑŒ Ð¼Ð¾Ð´ÐµÐ»ÐµÐ¹ Ð½Ðµ Ð´Ð°Ð» ÑÑƒÑ‰ÐµÑÑ‚Ð²ÐµÐ½Ð½Ð¾ Ð»ÑƒÑ‡ÑˆÐµÐ³Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°, Ð½Ð¾ Ð¿Ñ€Ð¸ ÑÑ‚Ð¾Ð¼ Ð½Ð°Ñ…Ð¾Ð´Ð¸Ñ‚ÑÑ Ð±Ð»Ð¸Ð¶Ðµ Ðº ÑÐµÑ€ÐµÐ´Ð¸Ð½Ðµ (Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ Ð¾ÑÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸) ÑÐ¾ Ð·Ð½Ð°Ñ‡ÐµÐ½Ð¸ÐµÐ¼ Accuracy: 0.614286\n",
        "ÐœÐžÐ´ÐµÐ»ÑŒ Ñ…Ð¾Ñ€Ð¾ÑˆÐ¾ Ð¾Ñ‚Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ð´Ð»Ð¸Ð½Ð½Ñ‹Ðµ Ð¸ ÐºÐ¾Ñ€Ð¾Ñ‚ÐºÐ¸Ðµ Ñ‚ÐµÐºÑÑ‚Ñ‹ Ð¸ ÑÐ»Ð¾Ð¶Ð½ÑƒÑŽ Ð»ÐµÐºÑÐ¸ÐºÑƒ( Ð±Ð»Ð°Ð³Ð¾Ð´Ð¾Ñ€Ñ Ð»ÐµÐ²ÐµÐ» Ð¸ Ð»ÐµÐ½Ð³Ð²Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ°Ð¼). Ð’Ð¾Ð·Ð¼Ð¾Ð¶Ð½Ð¾ Ñ‚Ñ€ÐµÐ±ÑƒÐµÑ‚ÑÑ Ð¿Ð¾Ñ€Ð°Ð±Ð¾Ñ‚Ð°Ñ‚ÑŒ Ñ Ð»Ð¸Ð½Ð³Ð¸Ð²ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ð¼Ð¸ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ°Ð¼Ð¸ Ð¸ n-Ð³Ñ€Ð°Ð¼Ð¼Ð°Ð¼Ð¸."
      ],
      "metadata": {
        "id": "XfQr9vIKDUQv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5hHFTPvzEc9G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}